version: '3'

services:

  server:
    build:
      context: server-code
      args:
        - MODEL_NAME=identity
        - SERVING_PORT=8500
    environment:
      - MODEL_NAME=identity
      - SERVING_PORT=8500
    ports:
      - "8500:8500"

  wsgi-client:
    build:
      context: client-code
      args:
        - CLIENT_PORT=8000
    command:
      gunicorn --workers=4 --bind=0.0.0.0:8000 wsgi_client:api
    environment:
      - MODEL_NAME=identity
      - SERVING_HOST=server
      - SERVING_PORT=8500
    ports:
      - "8000:8000"
    links:
      - server

  tornado-client:
    build:
      context: client-code
      args:
        - CLIENT_PORT=8001
    command:
      python3 tornado_client.py --client_port=8001
    environment:
      - MODEL_NAME=identity
      - SERVING_HOST=server
      - SERVING_PORT=8500
    ports:
      - "8001:8001"
    links:
      - server

  grpc-benchmark:
    build:
      context: client-code
    command:
      python3 grpc_client.py --num_requests=10000 --max_concurrent=10
    environment:
      - MODEL_NAME=identity
      - SERVING_HOST=server
      - SERVING_PORT=8500
    links:
      - server

  fastapi-client:
    build:
      context: client-code
      args:
        - CLIENT_PORT=8002
    command:
      uvicorn --workers=4 --host 0.0.0.0 --port 8002 fastapi_client:app
    environment:
      - MODEL_NAME=identity
      - SERVING_HOST=server
      - SERVING_PORT=8500
    ports:
      - "8002:8002"
    links:
      - server

  wsgi-benchmark:
    build:
      context: client-code
    command:
      ab -n 10000 -c 10 http://wsgi-client:8000/prediction
    links:
      - server
      - wsgi-client

  tornado-benchmark:
    build:
      context: client-code
    command:
      ab -n 10000 -c 10 http://tornado-client:8001/prediction
    links:
      - server
      - tornado-client
  
  fastapi-benchmark:
    build:
      context: client-code
    command:
      ab -n 10000 -c 10 http://tornado-client:8001/prediction
    links:
      - server
      - fastapi-client